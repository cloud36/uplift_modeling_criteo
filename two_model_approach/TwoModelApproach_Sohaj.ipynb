
# coding: utf-8

# In[1]:


import google.datalab.bigquery as bq
import google.datalab.storage as storage
from google.datalab import Context
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import clear_output
from scipy import stats

import xgboost
from sklearn.model_selection import *
from sklearn.metrics import *
from sklearn.calibration import *
from sklearn.ensemble import *
from sklearn import svm
from sklearn.svm import *
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression

import pylift
from pylift.pylift.eval import UpliftEval

import warnings
warnings.filterwarnings('ignore')


# ### Data Pull

# ##### Test: Pull 500K records from the test dataset and build out a dataframe

# In[2]:


get_ipython().run_cell_magic('bq', 'tables list', '')


# In[3]:


test_query="""
select *
from  `fit-reference-229502.test.test`
order by rand()
limit 500000
"""


# In[4]:


test = bq.Query(test_query).execute().result().to_dataframe()


# In[5]:


test.head()


# In[6]:


test.columns = ['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','treatment','conversion','visit','exposure']


# In[7]:


test.head()


# In[8]:


test.describe()


# In[9]:


test = test.apply(pd.to_numeric)


# In[10]:


test.describe()


# In[11]:


test = test.iloc[:, 0:14]


# In[12]:


test.head()


# ##### Train: Pull 1.5MM records from the train dataset and build out a dataframe

# In[13]:


sampling_query="""
select *
from  `fit-reference-229502.train.train`
order by rand()
limit 1500000
"""


# In[14]:


rdf = bq.Query(sampling_query).execute().result().to_dataframe()


# In[15]:


rdf.head()


# In[16]:


rdf.columns = ['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10','f11','treatment','conversion','visit','exposure']


# In[17]:


rdf.head()


# In[18]:


rdf = rdf.apply(pd.to_numeric)


# In[19]:


rdf.head()


# In[20]:


df = rdf.iloc[:, 0:14]


# In[21]:


df.head()


# ### Exploratory Analysis

# In[22]:


df.describe()


# In[23]:


df_f = df.iloc[:, 0:12]
df_f.head()


# In[24]:


df_f.describe()


# In[25]:


plt.figure(figsize = (20,10));
df_f.boxplot()


# In[26]:


df_f.hist(figsize =(20,10))


# In[27]:


corr = df.corr()
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(10,220, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})


# ###### Min-Max normalization of the dataset

# In[28]:


df=(df-df.min())/(df.max()-df.min())
df.head()


# In[29]:


df.describe()


# In[30]:


df.hist(figsize =(20,10))


# In[31]:


corr = df.corr()
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(10,220, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})


# ### Two Model Approach

# ##### Setup test/train datasets

# In[32]:


base = df[df.treatment == 0]
variant = df[df.treatment == 1]

delta = variant.conversion.mean() - base.conversion.mean()
delta_err = 1.96 * np.sqrt(variant.conversion.var() / variant.shape[0] + base.conversion.var() / base.shape[0])

print("Control: %s, N = %s, Conversion Rate: %s" % ( df[df.treatment == 0]['conversion'].sum() 
                               , df[df.treatment == 0].shape[0]
                               , df[df.treatment == 0]['conversion'].sum() / float(df[df.treatment == 0].shape[0]
     )))

print("Treatment: %s, N = %s, Conversion Rate: %s" % (df[df.treatment == 1]['conversion'].sum()
                            , df[df.treatment == 1].shape[0]
                            , df[df.treatment == 1]['conversion'].sum() / float(df[df.treatment == 1].shape[0]
     )))

print("")
print("Estimated Effect %s, Standard Error: %s" % (delta, delta_err))
print("")
lift = (variant.conversion.mean() - base.conversion.mean()) / base.conversion.mean()
print("Lift: %s" % lift)
print("")
t = stats.ttest_ind(df[df.treatment == 1]['conversion'], df[df.treatment == 0]['conversion'])
print("Test Statistic: %s, P-Value: %s" % (t[0], t[1]))


# In[33]:


# subset datasets
X_train_t, y_train_t = df[['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9',
                           'f10','f11']][df.treatment == 1],df['conversion'][df.treatment == 1]
X_train_c, y_train_c = df[['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9',
                           'f10','f11']][df.treatment == 0],df['conversion'][df.treatment == 0]

X_test, y_test = test[['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9',
                           'f10','f11']],test[['conversion','treatment']]


# In[35]:


print ("X_train_t Length: %s" % len(X_train_t))
print ("X_train_c Length: %s" % len(X_train_c))
print ("y_train_t Length: %s" % len(y_train_t))
print ("y_train_c Length: %s" % len(y_train_c))
print ("")
print ("y_test Length: %s" % len(y_test))
print ("y_test Length: %s" % len(y_test))


# In[36]:


X_test.head()


# In[37]:


y_test.head()


# ##### Run Models

# In[43]:


# SVM Model

#Model setup
svm = LinearSVC(random_state=0, class_weight = 'balanced')
clf_svm= CalibratedClassifierCV(svm)

# treatment
fit_t_svm = clf_svm.fit(X_train_t, y_train_t)
pt_pred_svm = fit_t_svm.predict_proba(X_test)[:,1]
yt_pred_svm1 = fit_t_svm.predict(X_test)
yt_pred_svm = [round(value) for value in yt_pred_svm1]

accuracy = accuracy_score(y_test[['conversion']], yt_pred_svm)
print("Treatment Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yt_pred_svm))
print(confusion_matrix(y_test[['conversion']], yt_pred_svm))

# control
fit_c_svm = clf_svm.fit(X_train_c, y_train_c)
pc_pred_svm = fit_c_svm.predict_proba(X_test)[:,1]
yc_pred_svm1 = fit_c_svm.predict(X_test)
yc_pred_svm = [round(value) for value in yc_pred_svm1]

accuracy = accuracy_score(y_test[['conversion']], yc_pred_svm)
print("Control Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yc_pred_svm))
print(confusion_matrix(y_test[['conversion']], yc_pred_svm))

# uplift 
prob_t_svm = pd.DataFrame(pt_pred_svm)
prob_c_svm = pd.DataFrame(pc_pred_svm)
u_svm = pd.merge(prob_t_svm, prob_c_svm, left_index = True, right_index = True)
u_svm.columns = ['prob_t_svm','prob_c_svm']
u_svm['uplift_svm'] = prob_t_svm - prob_c_svm


# In[44]:


# results dataframe
results1 = pd.merge(y_test, u_svm, left_index=True, right_index=True)


# In[45]:


results1.head()


# In[46]:


# Logistic Regression

# grid search paramaters
params = {'C':[1, 10, 100]}

#Model setup
lr=LogisticRegression(random_state=0, class_weight='balanced')
clf_lr= GridSearchCV(lr, params)

# treatment
fit_t_lr = clf_lr.fit(X_train_t, y_train_t)
print('Best Treatment Parameters:', fit_t_lr.best_params_)
pt_pred_lr = fit_t_lr.predict_proba(X_test)[:,1]
yt_pred_lr1 = fit_t_lr.predict(X_test)
yt_pred_lr = [round(value) for value in yt_pred_lr1]

accuracy = accuracy_score(y_test[['conversion']], yt_pred_lr)
print("Treatment Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yt_pred_lr))
print(confusion_matrix(y_test[['conversion']], yt_pred_lr))

# control
fit_c_lr = clf_lr.fit(X_train_c, y_train_c)
print('Best Control Parameters:', fit_c_lr.best_params_)
pc_pred_lr = fit_c_lr.predict_proba(X_test)[:,1]
yc_pred_lr1 = fit_c_lr.predict(X_test)
yc_pred_lr = [round(value) for value in yc_pred_lr1]

accuracy = accuracy_score(y_test[['conversion']], yc_pred_lr)
print("Control Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yc_pred_lr))
print(confusion_matrix(y_test[['conversion']], yc_pred_lr))

# uplift 
prob_t_lr = pd.DataFrame(pt_pred_lr)
prob_c_lr = pd.DataFrame(pc_pred_lr)
u_lr = pd.merge(prob_t_lr, prob_c_lr, left_index = True, right_index = True)
u_lr.columns = ['prob_t_lr','prob_c_lr']
u_lr['uplift_lr'] = prob_t_lr - prob_c_lr


# In[47]:


# results dataframe
results2 = pd.merge(results1, u_lr, left_index=True, right_index=True)


# In[48]:


results2.head()


# In[53]:


# Random Forest

# grid search paramaters
params = {
    'n_estimators': [30,40,50] ,
    'max_depth': [10,20,30],
    'max_features': [2, 3]
}

#Model setup
rf = RandomForestClassifier(random_state=0, class_weight = 'balanced')
clf_rf= RandomizedSearchCV(rf, params)

# treatment
fit_t_rf = clf_rf.fit(X_train_t, y_train_t)
print('Best Treatment Parameters:', fit_t_rf.best_params_)
pt_pred_rf = fit_t_rf.predict_proba(X_test)[:,1]
yt_pred_rf1 = fit_t_rf.predict(X_test)
yt_pred_rf = [round(value) for value in yt_pred_rf1]

accuracy = accuracy_score(y_test[['conversion']], yt_pred_rf)
print("Treatment Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yt_pred_rf))
print(confusion_matrix(y_test[['conversion']], yt_pred_rf))

# control
fit_c_rf = clf_rf.fit(X_train_c, y_train_c)
print('Best Control Parameters:', fit_c_rf.best_params_)
pc_pred_rf = fit_c_rf.predict_proba(X_test)[:,1]
yc_pred_rf1 = fit_c_rf.predict(X_test)
yc_pred_rf = [round(value) for value in yc_pred_rf1]

accuracy = accuracy_score(y_test[['conversion']], yc_pred_rf)
print("Control Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yc_pred_rf))
print(confusion_matrix(y_test[['conversion']], yc_pred_rf))
      
# uplift 
prob_t_rf = pd.DataFrame(pt_pred_rf)
prob_c_rf = pd.DataFrame(pc_pred_rf)
u_rf = pd.merge(prob_t_rf, prob_c_rf, left_index = True, right_index = True)
u_rf.columns = ['prob_t_rf','prob_c_rf']
u_rf['uplift_rf'] = prob_t_rf - prob_c_rf


# In[54]:


# results dataframe
results3 = pd.merge(results2, u_rf, left_index=True, right_index=True)


# In[55]:


results3.head()


# In[58]:


# XGBoost

# grid search paramaters
params = {
        'max_depth': [3, 5, 7],
        'min_child_weight': [1, 3, 5],
        'gamma': [0.5, 1, 1.5]
}

#Model setup

spw=(y_train_t.shape[0] - y_train_t.sum()) / y_train_t.sum()
xg = xgboost.XGBClassifier(scale_pos_weight=spw)
clf_xg= RandomizedSearchCV(xg, params)

# treatment
fit_t_xg = clf_xg.fit(X_train_t, y_train_t)
print('Best Treatment Parameters:', fit_t_xg.best_params_)
pt_pred_xg = fit_t_xg.predict_proba(X_test)[:,1]
yt_pred_xg1 = fit_t_xg.predict(X_test)
yt_pred_xg = [round(value) for value in yt_pred_xg1]

accuracy = accuracy_score(y_test[['conversion']], yt_pred_xg)
print("Treatment Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yt_pred_xg))
print(confusion_matrix(y_test[['conversion']], yt_pred_xg))

# control
fit_c_xg = clf_xg.fit(X_train_c, y_train_c)
print('Best Control Parameters:', fit_c_xg.best_params_)
pc_pred_xg = fit_c_xg.predict_proba(X_test)[:,1]
yc_pred_xg1 = fit_c_xg.predict(X_test)
yc_pred_xg = [round(value) for value in yc_pred_xg1]

accuracy = accuracy_score(y_test[['conversion']], yc_pred_xg)
print("Control Accuracy: %.2f%%" % (accuracy * 100.0))
print(classification_report(y_test[['conversion']], yc_pred_xg))
print(confusion_matrix(y_test[['conversion']], yc_pred_xg))

# uplift 
prob_t_xg = pd.DataFrame(pt_pred_xg)
prob_c_xg = pd.DataFrame(pc_pred_xg)
u_xg = pd.merge(prob_t_xg, prob_c_xg, left_index = True, right_index = True)
u_xg.columns = ['prob_t_xg','prob_c_xg']
u_xg['uplift_xg'] = prob_t_xg - prob_c_xg
      


# In[59]:


results4 = pd.merge(results3, u_xg, left_index=True, right_index=True)


# In[60]:


results4.head()


# ##### Evaluation

# In[70]:


result = results4


# In[71]:


test = results4


# In[73]:


#Logistic Regression
test = test.sort_values(by=['uplift_lr'], ascending=False)
uplev_lr = UpliftEval(test['treatment'], test['conversion'], test['uplift_lr'] , n_bins=10)
print("Logistic Regression q1 qini score: ", uplev_lr.q1_qini)
uplev_lr.plot(plot_type='qini',show_practical_max=True, show_no_dogs=True, show_theoretical_max=True)


# In[72]:


#SVM
test = test.sort_values(by=['uplift_svm'], ascending=False)
uplev_svm = UpliftEval(test['treatment'], test['conversion'], test['uplift_svm'] , n_bins=10)
print("SVM q1 qini score: ", uplev_svm.q1_qini)
uplev_svm.plot(plot_type='qini',show_practical_max=True, show_no_dogs=True, show_theoretical_max=True)


# In[76]:


# Random Forest
test = test.sort_values(by=['uplift_rf'], ascending=False)
uplev_rf = UpliftEval(test['treatment'], test['conversion'], test['uplift_rf'] , n_bins=10)
print("Random Forest q1 qini score: ", uplev_rf.q1_qini)
uplev_rf.plot(plot_type='qini',show_practical_max=True, show_no_dogs=True, show_theoretical_max=True)


# In[77]:


#XGBoost
test = test.sort_values(by=['uplift_xg'], ascending=False)
uplev_xg = UpliftEval(test['treatment'], test['conversion'], test['uplift_xg'] , n_bins=10)
print("XGBoost q1 qini score: ", uplev_xg.q1_qini)
uplev_xg.plot(plot_type='qini',show_practical_max=True, show_no_dogs=True, show_theoretical_max=True)


# #### Save Results file in storage

# In[80]:


result.to_csv('results.csv', index = False)
get_ipython().system("gsutil cp 'results.csv' 'gs://fit-reference-229502/results.csv'")


# In[82]:


from io import BytesIO
get_ipython().run_line_magic('gcs', 'read --object gs://fit-reference-229502/results.csv --variable results_file')
df2 = pd.read_csv(BytesIO(results_file))


# In[83]:


df2.head()

